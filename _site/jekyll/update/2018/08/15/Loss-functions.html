<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Loss functions | Your awesome title</title>
<meta name="generator" content="Jekyll v3.8.3" />
<meta property="og:title" content="Loss functions" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Loss functions" />
<meta property="og:description" content="Loss functions" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2018/08/15/Loss-functions.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2018/08/15/Loss-functions.html" />
<meta property="og:site_name" content="Your awesome title" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-08-15T11:28:24+09:00" />
<script type="application/ld+json">
{"headline":"Loss functions","dateModified":"2018-08-15T11:28:24+09:00","datePublished":"2018-08-15T11:28:24+09:00","url":"http://localhost:4000/jekyll/update/2018/08/15/Loss-functions.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2018/08/15/Loss-functions.html"},"description":"Loss functions","@type":"BlogPosting","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Your awesome title" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Your awesome title</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Loss functions</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-08-15T11:28:24+09:00" itemprop="datePublished">Aug 15, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="loss-functions">Loss functions</h1>

<hr />

<p>딥러닝 하다보면 그냥 사용했던 <code class="highlighter-rouge">Loss Function</code>들에 대해서 한번 다뤄 본다.</p>

<h2 id="entropy">Entropy</h2>

<p>정보를 최적으로 인코딩하기 위해 필요한 <code class="highlighter-rouge">bit</code>의 수
$\log_{2}^{}{N}$</p>

<h3 id="요일-예시-uniform">요일 예시 (Uniform)</h3>

<p>만약 요일에 대한 정보를 메시지에 실어서 보내야 한다면 가장 최소의 bit를 사용한다면 얼마나 될까?
<code class="highlighter-rouge">Yes</code>/<code class="highlighter-rouge">No</code>수준이라면 그냥 1 bit만으로 충분하다.</p>

<p>그렇다면 econding을 day of the week을 해야한다고하면, 7 values를 encode 해야한다.
결국 $\log_{2}^{}{7}$
<code class="highlighter-rouge">월화수..일</code> 7가지를 bit로 전송하기 위해선 <code class="highlighter-rouge">3bit</code>필요 하게 된다.</p>
<ul>
  <li>bits (000 – Monday, 001 – Tuesday, …, 110 – Sunday, 111- unused).
    <blockquote>
      <p>bit의 발생 빈도가 uniform 하다는 가정에 의해서 발생</p>
    </blockquote>
  </li>
</ul>

<h3 id="speech-of-a-sequence-of-words-skewness">Speech of a sequence of words (skewness)</h3>

<p>만약 영어 단어를 말하는것을 encode 하는 것을 생각해 보자.
그냥 단순히 uniform하다고 가정하면 $\log_{2}^{}{40} = 5.3$ 이다.</p>

<p>하지만 4개의 <code class="highlighter-rouge">Tag</code>값들이 <code class="highlighter-rouge">90%</code>가 발생한다면 좀 더 효율적인 <code class="highlighter-rouge">bit encode</code>를 만들어 낼 수 있다.</p>

<p>40개의 문자가 있고 상위 4개가 90%로 발생한다.</p>
<ul>
  <li><code class="highlighter-rouge">ART</code>, <code class="highlighter-rouge">P</code>, <code class="highlighter-rouge">N</code>, <code class="highlighter-rouge">V</code></li>
</ul>

<p>아이디어는 첫 번째 <code class="highlighter-rouge">bit</code>를 단순히 위 네개의 TAG를 구분하는데 사용 한다.</p>
<ul>
  <li>YES 이면 추가로 <code class="highlighter-rouge">2bit</code>를 더 필요로 해서 구분하게 된다.</li>
  <li>NO 이면 <code class="highlighter-rouge">6bit</code>를 더 필요로 한다. <code class="highlighter-rouge">40-4 = 36</code>이고, $\log_{2}^{}{36}=5.16$</li>
</ul>

<p>정리하면 아래 표와 같다.
<img src="https://i.imgur.com/Kuvrqxr.jpg" alt="" /></p>

<p>이것을 생각한 방식으로 계산하면 아래와 같다.
필요한 bit수 계산 = $0.9 \times 3 + 0.1 \times 7 = 3.4$</p>

<p>원래 아이디어 보다 <code class="highlighter-rouge">bit</code>수가 더 줄어 들었다. 거의 반만큼.
여기서 더 최적화 할 수 있을까?</p>

<p>Information theory provides an answer.</p>
<blockquote>
  <p>As mentioned before, <code class="highlighter-rouge">Entropy</code> is a measure of randomness in a probability distribution.
A central theorem of information theory states that the entropy of <code class="highlighter-rouge">p</code> specifies the minimum number of bits needed to encode the values of a random variable <code class="highlighter-rouge">X</code> with probability function <code class="highlighter-rouge">p</code>.</p>
</blockquote>

<h3 id="definition-of-entropy">Definition of Entropy</h3>

<p><code class="highlighter-rouge">X</code>를 <code class="highlighter-rouge">random variable</code>이라고 하면 <code class="highlighter-rouge">p</code>는 probability function이다. 
<script type="math/tex">p(x_{i}) = P(X=x_{i})</script>
이라고 한다 (보통 algebra variable과 혼동하지 않기 위해서 capital로 표기하지만 여기선 간소화를 위해서 assumption을 만들고 시작).</p>

<p>Entropy of X (or p)의 정의는 아래와같다.</p>

<p>$H(X) = H(p) = - \sum_{i}{p(x_i) \times \log{p(x_i)} }$, where $x_{i}$ ranges over the vocabulary of $X$</p>

<p>위 식으로 다시 계산해보면 TAG <code class="highlighter-rouge">random variable</code>에 대해서 actual probability distribution을 표1에서와 같이 알기 때문에 TAG의 entropy는 아래와 같이 계산된다.</p>

<p>$ H(TAG) = -(4 \times (.225 \times \log_{2}{.225}) + 36 \times (.0028 \times \log_{2}{.0028})) = -(-1.04 + -.82 + -.85) = 2.72 $</p>

<p>결과 값을 통해서 그냥 단순하게 coding 했던것 보다 좋았던 <code class="highlighter-rouge">3.4</code>보다 더 낮은 <code class="highlighter-rouge">2.72</code>가 optimal 이라는것을 알게 되었다.
하지만, <code class="highlighter-rouge">entropy</code>의 문제는 이게 가장 best한 conding 디자인이 아니라는것만 알려주지 실제로 그래서 어떻게 coding 해야 optimal에 도달하는지를 알려주지 않는다. 더 나아가 physically이게 가능한 것인지도 알 수 없다.</p>

<p><strong>Data science에서 보는 값에 따른 해석</strong></p>
<ul>
  <li><code class="highlighter-rouge">High Entropy</code>는 x가 uniform한 distribution을 가지게 된다. 즉 boring하다고 해석할수도 있고 diversity가 높다고 할수도 있다. Active Learning에서 그래서 이러한 Entropy를 이용해서 smapling을 할지 말지를 고려할 수도 있다.</li>
  <li><code class="highlighter-rouge">Low Entropy</code>는 skewness가 높다고 할 수 있다. 결국 varied distribution이라 할 수 있고 <code class="highlighter-rouge">peak</code>나 <code class="highlighter-rouge">valley</code>가 많다고 할 수 있다.</li>
</ul>

<h2 id="cross-entropy">Cross-Entropy</h2>

<p>두개의 probability function을 비교하는것이 cross entropy이다.</p>
<ul>
  <li><code class="highlighter-rouge">p</code>와 <code class="highlighter-rouge">q</code>로 구분한다.
이러한 <code class="highlighter-rouge">Cross Entropy</code>는 symmetric하지 않기 때문에 그 역은 성립하지 않는다.
<code class="highlighter-rouge">p</code>를 보통 target으로 <code class="highlighter-rouge">q</code>를 estimated one으로 설정한다.
그렇게 비교해서 얼마나 서로 가까운지를 알아내서 그것을 최적화하는 방법으로 학습에 사용한다.</li>
</ul>

<script type="math/tex; mode=display">H(p,q)= -\sum _{ i }{ p(x_{ i }) } \log { q(x_{ i }) }</script>

<p>$p_{y=1} = y$ 이고 $p_{y=0} = 1-y$ 라면,  $p \in (y, 1-y), q \in (\hat{y} ,1-\hat{y})$</p>

<script type="math/tex; mode=display">H(p,q)= -\sum _{ i }{ p(x_{ i }) } \log { q(x_{ i }) } = -y\ln{\hat{y}} + (1-y)\ln(1-\hat{y})</script>

<p><strong>MSE의 문제점</strong>
<script type="math/tex">MSE = C = \frac{2}</script>
<script type="math/tex">a = \sigma(z)</script>
<script type="math/tex">z = wx+b</script>
위의 것을 미분하면,
<script type="math/tex">\frac{\partial C}{\partial w} = (a-y)\sigma^{\prime}  {(z)} x</script>
$\sigma^{\prime}  {(z)}$ 값이 최대 <code class="highlighter-rouge">0.25</code>가 나오고 대부분 0이기 때문에 vanishing gradient가 생기는건 잘알려진 사실이다.</p>

<p><strong>cross entropy 미분</strong></p>

<script type="math/tex; mode=display">C= -\frac{1}{n} \sum_{x}^{}{y\ln{a} + (1-y)\ln(1-a)}</script>

<script type="math/tex; mode=display">\frac { \partial C }{ \partial w_{ j } } =-\frac { 1 }{ n } \sum _{ x }{ (\frac { y }{ \sigma (z) } -\frac { (1-y) }{ 1-\sigma (z) } ) } \frac { \partial \sigma  }{ \partial w_{ j } } \\ =-\frac { 1 }{ n } \sum _{ x }{ (\frac { y }{ \sigma (z) } -\frac { (1-y) }{ 1-\sigma (z) } ) } \sigma ^{ \prime  }(z)x_{ j }\\ =\frac { 1 }{ n } \sum _{ x }{ \frac { \sigma ^{ \prime  }(z)x_{ j } }{ \sigma (z)(1-\sigma (z)) }  } \sigma ^{ \prime  }(z)-y</script>

<p>최종적으로 아래식 처럼 $\sigma(z)$가 살아있기 때문에 vanishing 문제를 적게 발생 시키는 좋은 폼을 형성한다.
<script type="math/tex">\frac{\partial C}{\partial w_{j}} = \frac{1}{n} \sum_{x}^{}{x_{j}(\sigma(z)-y) }</script></p>

<h2 id="kl-divergence">KL-divergence</h2>

<p>두 분포 차이를 줄이기 위해서는 <code class="highlighter-rouge">KL-Divergence</code>를 사용한다.</p>

<script type="math/tex; mode=display">D_{KL}(P \parallel Q) = -\sum_{x}^{}{p(x) \log{q(x)}} + \sum_{x}^{}{p(x) \log{p(x)}}</script>

<script type="math/tex; mode=display">= H(P,Q) - H(P)</script>

<h2 id="참고자료">참고자료</h2>
<p><a href="http://www.cs.rochester.edu/u/james/CSC248/Lec6.pdf">Lecture6: Using Entropy for Evaluating and Comparing Probability Distributions</a></p>

<p><a href="https://www.youtube.com/watch?v=zJmbkp9TCXY&amp;list=PL0oFI08O71gKEXITQ7OG2SCCXkrtid7Fq&amp;index=21">정보량을 나타내는 앤트로피(Entropy)</a></p>

<p><a href="https://www.youtube.com/watch?v=uMYhthKw1PU">4.1. Cross entropy와 KL divergence</a></p>

  </div><a class="u-url" href="/jekyll/update/2018/08/15/Loss-functions.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Your awesome title</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Your awesome title</li><li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
